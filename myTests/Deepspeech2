#!/usr/bin/env python3

# Copyright (c) 2020, Soohwan Kim. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import torch.nn as nn
import time
import cProfile
import argparse
import subprocess
import threading
import re
import math
import pstats
import io

from kospeech.models import DeepSpeech2


class TegrastatsMonitor:
    def __init__(self, interval=5):
        self.interval = interval
        self.running = False
        self.thread = None
        self.max_min = {
            'mem_used_MB': {
                'max': -1,
                'min': math.inf
            },
            'mem_total_MB': None,
            'power_mW_current': {
                'max': -1,
                'min': math.inf
            },
            'power_mW_avg': {
                'max': -1,
                'min': math.inf
            },
            'gpu_utilization': {
                'max': -1,
                'min': math.inf
            },
            'gpu_temperature': {
                'max': -1,
                'min': math.inf
            }
        }

    def parse_tegrastats_output(self, output):
        # Example line:
        # RAM 1024/3964MB (lfb 123x4MB) SWAP 0/1982MB CPU [5%@345,off,3%@345,...] EMC_FREQ 1%@1600 GR3D_FREQ 0%@998 VDD_IN 1234/1234 VDD_CPU_GPU_CV 567/567
        mem_match = re.search(r'RAM (\d+)/(\d+)MB', output)
        power_match = re.search(r'VDD_IN (\d+)mW/(\d+)mW', output)
        utilization_match = re.search(r'GR3D_FREQ (\d+)%', output)
        temperature_match = re.search(r'gpu@(\d+\.\d+)C', output)

        mem_used = int(mem_match.group(1)) if mem_match else None
        mem_total = int(mem_match.group(2)) if mem_match else None
        power_current = int(power_match.group(1)) if power_match else None
        power_avg = int(power_match.group(2)) if power_match else None
        gpu_usage = int(utilization_match.group(1)) if utilization_match else None
        gpu_temp = float(temperature_match.group(1)) if temperature_match else None

        return_dict = {
            'mem_used_MB': mem_used,
            'mem_total_MB': mem_total,
            'power_mW_current': power_current,
            'power_mW_avg': power_avg,
            'gpu_utilization': gpu_usage,
            'gpu_temperature': gpu_temp
        }

        self.max_min["mem_total_MB"] = mem_total

        for key, value in return_dict.items():
            if key == "mem_total_MB": continue
            if not value: continue

            if value > self.max_min[key]["max"]:
                self.max_min[key]["max"] = value
            if value < self.max_min[key]["min"]:
                self.max_min[key]["min"] = value
        
        return return_dict

    def _monitor(self):
        print("[Tegrastats] Continuous monitoring started.")
        try:
            proc = subprocess.Popen(['tegrastats'], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, text=True)
            while self.running:
                line = proc.stdout.readline()
                if not line:
                    break
                stats = self.parse_tegrastats_output(line.strip())
                print(f"[Tegrastats] Stats: {stats}")
                time.sleep(self.interval)  # Optional: throttle processing
            proc.terminate()
        except Exception as e:
            print(f"[Tegrastats] Error: {e}")

    def start(self):
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitor, daemon=True)
            self.thread.start()

    def stop(self):
        if self.running:
            self.running = False
            self.thread.join()
            print("[Tegrastats] Monitoring stopped.")



def func(batch_size):
    sequence_length = 14321
    dimension = 80

    cuda = torch.cuda.is_available()
    device = torch.device('cuda' if cuda else 'cpu')

    print(device)

    model = DeepSpeech2(num_classes=10, input_dim=dimension).to(device)

    criterion = nn.CTCLoss(blank=3, zero_infinity=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-04)

    # Define input lengths, targets, and target lengths based on batch size
    if batch_size == 2:
        input_lengths = torch.IntTensor([12345, 12300]).to(device)
        targets = torch.LongTensor([[1, 3, 3, 3, 3, 3, 4, 5, 6, 2],
                                     [1, 3, 3, 3, 3, 3, 4, 5, 2, 0]]).to(device)
        target_lengths = torch.LongTensor([9, 8]).to(device)
    elif batch_size == 3:
        input_lengths = torch.IntTensor([12345, 12300, 12000]).to(device)
        targets = torch.LongTensor([[1, 3, 3, 3, 3, 3, 4, 5, 6, 2],
                                     [1, 3, 3, 3, 3, 3, 4, 5, 2, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0]]).to(device)
        target_lengths = torch.LongTensor([9, 8, 7]).to(device)
    elif batch_size == 4:
        input_lengths = torch.IntTensor([12345, 12300, 12000, 12000]).to(device)
        targets = torch.LongTensor([[1, 3, 3, 3, 3, 3, 4, 5, 6, 2],
                                     [1, 3, 3, 3, 3, 3, 4, 5, 2, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0]]).to(device)
        target_lengths = torch.LongTensor([9, 8, 7, 6]).to(device)
    elif batch_size == 5:
        input_lengths = torch.IntTensor([12345, 12300, 12000, 12000, 12000]).to(device)
        targets = torch.LongTensor([[1, 3, 3, 3, 3, 3, 4, 5, 6, 2],
                                     [1, 3, 3, 3, 3, 3, 4, 5, 2, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0],
                                     [1, 3, 3, 3, 3, 3, 4, 2, 0, 0]]).to(device)
        target_lengths = torch.LongTensor([9, 8, 7, 6, 5]).to(device)
    else:
        raise ValueError("Batch size must be between 2 and 5.")

    for i in range(10):
        inputs = torch.rand(batch_size, sequence_length, dimension).to(device)
        outputs, output_lengths = model(inputs, input_lengths)

        loss = criterion(outputs.transpose(0, 1), targets[:, 1:], output_lengths, target_lengths)
        loss.backward()
        optimizer.step()
        print(loss)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Benchmark model training with specified batch size.')
    parser.add_argument('batch_size', type=int, choices=[2, 3, 4, 5], help='Batch size for training (must be 2, 3, 4, or 5).')

    args = parser.parse_args()

    monitor = TegrastatsMonitor(interval=1)
    pr = cProfile.Profile()

    monitor.start()
    pr.enable()
    func(args.batch_size)
    pr.disable()
    monitor.stop()

    s = io.StringIO()
    sortby = 'cumulative'
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)

    output_dict = monitor.max_min

    output_dict["total_time"] = ps.total_tt

    ps.print_stats()
    profiling_output = s.getvalue()
    print(profiling_output)

    backward_pattern = r'\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s__init__\.py:165\(backward\)'
    backward_match = re.search(backward_pattern, profiling_output, re.MULTILINE)
    if backward_match:
        output_dict['backward_time'] = backward_match.group(4)
    
    forward_pattern = r'\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\s+(-?\d+(?:\.\d+)?)\smodel\.py:142\(forward\)'
    forward_match = re.search(forward_pattern, profiling_output, re.MULTILINE)
    if forward_match:
        output_dict['forward_time'] = forward_match.group(4)
    
    values = []
    for key, value in data.items():
        if isinstance(value, dict):
            values.append(value['max'])
            values.append(value['min'])
        else:
            values.append(value)
    
    print(', '.join(map(str, values)))
